{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Day-ahead load prediction for ERCOT (Texas) markets. \n",
    "\n",
    "In this lab, you train a neural network to predict 24-hour aggregate load from Texas for a day using history of demands. The goals for this lab are:\n",
    "1. Load the data and analyze to find patterns.\n",
    "2. Define a neural network for the regression. Try different number of layers, learning rates, linear v/s nonlinear regression, activation functions, number of epochs, etc.\n",
    "3. Explore the effects of wind energy on load prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The following line suppresses certain warnings.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the ERCOT data from 2015.\n",
    "\n",
    "The load data is given in the column named 'ERCOT Load, MW' in the csv file provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2015\n",
    "dfDemand = pd.read_csv(\"ERCOT_Hourly_Wind_Output_\" + str(year) + \".csv\")\n",
    "\n",
    "demands = dfDemand['ERCOT Load, MW']\n",
    "\n",
    "# Count the number of days for which we have demand data.\n",
    "numberOfDays = int(len(demands)/24)\n",
    "print(\"Hourly demand data loaded for %d days.\" % numberOfDays)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the data.\n",
    "\n",
    "It is always useful to get accustomed to the data you are trying to learn. Visualize it if you can.\n",
    "\n",
    "#### Q1. How does load vary over the year in Texas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "plt.plot([hour/24 for hour in range(numberOfDays * 24)], demands.values)\n",
    "plt.xlabel(\"Days in \" + str(year))\n",
    "plt.ylabel(\"Net demand of Texas (in MW)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fact.** A significant portion of the demand is usually thermal, i.e., for air conditioners and heating systems.\n",
    "\n",
    "**Question (5 points).** From the above plot, what can you infer about the climate of Texas? What would you expect if you plotted the same in Illinois? \n",
    "\n",
    "**Your answer.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2. How does day of week affect the load profiles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the load data of the same day of the week over several weeks.\n",
    "\n",
    "dayStart = 30\n",
    "numberOfWeeks = 4\n",
    "\n",
    "DayOfWeek = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "print(\"The first day in the first plot is Jan 31, \" + str(year) + \".\")\n",
    "print(\"Day 1\", \"was a\", DayOfWeek[datetime.date(year, 1, 31).weekday()] + \".\")\n",
    "\n",
    "fig, axs = plt.subplots(7, 1, sharex=True, figsize=(5,10))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for dayInFirstWeek in range(7):\n",
    "    for week in range(numberOfWeeks):\n",
    "\n",
    "        axs[dayInFirstWeek].plot(range(24), dfDemand.loc[(dayStart + 7 * week + dayInFirstWeek) * 24: \n",
    "                                                         (dayStart + 7 * week + dayInFirstWeek + 1) * 24 - 1, \n",
    "                                                         'ERCOT Load, MW'].values.flatten())\n",
    "    axs[dayInFirstWeek].set_ylim(bottom=20000, top=60000)\n",
    "    axs[dayInFirstWeek].set_title(\"Day \" + str(dayInFirstWeek + 1))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question (5 points).** Can you find any discernible change in the load profiles of different days of the week?\n",
    "\n",
    "**Your answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question (15 points).** Redo the above exercise for the month of August. Make 'Day 1' correspond to August 15th. What do you observe differently? Do your observations agree with Q1? \n",
    "\n",
    "**Your answer (comments here, code below).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the demand prediction module.\n",
    "\n",
    "Use past demand profiles to predict demands a day in advance. We draw two conclusions from the above analysis:\n",
    "1. Demand profiles have seasonal effects. Therefore, data from the past few days will help in predicting the demands tomorrow.\n",
    "2. Demand profiles have weekly dependencies. Therefore, data from the same days but a week or two before can be useful in load prediction.\n",
    "\n",
    "How much past data you want to train over depends on two considerations:\n",
    "1. Which data in the past is useful in prediction?\n",
    "2. How complex you want your training process to be? The more features of past data you want to train on, the more complex your neural network should be, and it will require more time to train it.\n",
    "\n",
    "To strike a balance, use the demand profile from $d-7, d-2, d-1$ to predict the load profile of day $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daysToTrainOn = [-7, -2, -1]\n",
    "rangeOfDays = range(-np.min(daysToTrainOn), numberOfDays)\n",
    "\n",
    "X = [np.concatenate([dfDemand.loc[(day + h) * 24: (day + h + 1) * 24 -1, 'ERCOT Load, MW'].values.flatten()\n",
    "     for h in daysToTrainOn]) for day in rangeOfDays]\n",
    "Y = [dfDemand.loc[day * 24: (day + 1) * 24 - 1, 'ERCOT Load, MW'].values.flatten() for day in rangeOfDays]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you perform regression, it is often desirable to scale the inputs so that it has zero mean and unit variance. Other types of scaling are possible. Here, we cheat a little and scale both the training and test data together. Ideally, they should be scaled separately.\n",
    "\n",
    "Split the data into two sets: training set and testing set. Train the neural network on the training set, and test how well it performs on the testing set. You should typically never sample from the training set to test your algorithms. The learnt model for prediction should work well on data that the algorithm has never encountered before.\n",
    "\n",
    "The function 'train_test_split' helps you to split the data into two parts, where 'test_size'\n",
    "indicates the fraction of the data you want to test on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessing.StandardScaler().fit_transform(X)\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "print(\"Scaled and split the data into two parts:\")\n",
    "\n",
    "nTrain = np.shape(trainX)[0]\n",
    "nTest = np.shape(testX)[0]\n",
    "\n",
    "print(\"Neural network will train on data from %d days, and test on %d days.\" % (nTrain, nTest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design the neural network (NN) for demand prediction with only one hidden layer.\n",
    "\n",
    "Recall that TensorFlow defines a computation graph where the weights and biases associated with the NN are variables. The goal is to optimize the weights and biases of the NN to minimize prediction error using data. \n",
    "\n",
    "\n",
    "To define the computation graph, create the inputs and outputs as 'placeholders'.\n",
    "The algorithm only expects them to be specified at the time of computation. The\n",
    "first element of the shape attribute for both inputs and outputs are 'None'. This\n",
    "means that they are left unspecified, and will be provided at runtime. It will help\n",
    "in batch training for prediction, where the size of the batch will determine\n",
    "this value. Batch training is useful because training the NN with one data point at a\n",
    "time can be time consuming.\n",
    "\n",
    "In this lab, we begin with a 'relu' activation. We additionally implement 'dropouts' that basically\n",
    "prevents certain paramters from updating in each round. This is known to prevent overfitting. The number'0.995' in the description below updates 99.5% of all weights, leaving out 0.5%.\n",
    "\n",
    "Design the optimizer and the loss. For reporting the accuracy of prediction, we choose in this lab the idea of mean absolute error (MAE). For a data set, if the true values are scalars $y_1, \\ldots, y_m$ and the predictions are $\\hat{y}_1, \\ldots, \\hat{y}_m$, then its MAE is given by\n",
    "$$ MAE = \\frac{1}{m}\\sum|y_i - \\hat{y}_i|.$$\n",
    "If $y$ and $\\hat{y}$ are multidimensional, it computes the average across each coordinate of $y$ and $\\hat{y}$.\n",
    "\n",
    "**Question (5 points). Insert a line of code for the output of layer 1 below (use the relu function)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nHidden = 150\n",
    "\n",
    "# Store the dimension of each row of 'X' in 'nDimX' and that of 'Y' in 'nDimY' .\n",
    "nDimX = np.shape(trainX)[1]\n",
    "nDimY = np.shape(trainY)[1]\n",
    "\n",
    "# Define the inputs and the target outputs for the NN.\n",
    "inputNN = tf.placeholder(dtype=tf.float32, shape=[None, nDimX])\n",
    "targetOutputNN = tf.placeholder(dtype=tf.float32, shape=[None, nDimY])\n",
    "\n",
    "# Define the weights and biases of the first layer.\n",
    "W1 = tf.Variable(tf.truncated_normal(shape=[nDimX, nHidden]))\n",
    "b1 = tf.Variable(tf.zeros(nHidden))\n",
    "\n",
    "# Define the output of layer 1. \n",
    "## use the function 'tf.nn.relu' to define 'OutputLayer1'.  \n",
    "\n",
    "# ....\n",
    "\n",
    "outputLayer1 = tf.nn.dropout(outputLayer1, 0.995)\n",
    "\n",
    "# Define the weights and biases of the second layer.\n",
    "W2 = tf.Variable(tf.truncated_normal(shape=[nHidden, nDimY]))\n",
    "b2 = tf.Variable(tf.zeros(nDimY))\n",
    "\n",
    "# Define the output of layer 2.\n",
    "outputNN = tf.nn.dropout((tf.matmul(outputLayer1, W2) + b2), 0.995)\n",
    "\n",
    "# Define the loss function and the optimizer.\n",
    "loss = tf.losses.mean_squared_error(labels=targetOutputNN, predictions=outputNN)\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=0.25).minimize(loss)\n",
    "\n",
    "# Compute the MAE metric to judge accuracy of prediction.\n",
    "_, maeY = tf.metrics.mean_absolute_error(labels=targetOutputNN, predictions=outputNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Train the neural network.\n",
    "\n",
    "Create the training module for the NN. Feed the training data in batches of size 'batchSize'\n",
    "and ask Tensorflow to run the function 'optimizer'. The number of batches, denoted by 'nBatches'\n",
    "is then given by the size of your training dataset divided by 'batchSize. Usually, going through\n",
    "the training data once does not train your NN. You train over the same data multiple\n",
    "times. More precisely, train it 'nEpochs' times. It is similar to the idea that you never learn\n",
    "a material by reading through it once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 50\n",
    "nBatches = int(nTrain/batchSize)\n",
    "nEpochs = 5000\n",
    "\n",
    "# Define a session.\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "\n",
    "    # Initialize the computation graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    print(\"Started the training module.\")\n",
    "\n",
    "    for epoch in range(nEpochs):\n",
    "\n",
    "        lossEpoch = 0\n",
    "        # In each epoch, use 'optimizer' to reduce the 'loss' over batches of data.\n",
    "        for n in range(nBatches):\n",
    "            # Define the batch to train on.\n",
    "            batchX = trainX[n * batchSize: (n + 1) * batchSize]\n",
    "            batchY = trainY[n * batchSize: (n + 1) * batchSize]\n",
    "\n",
    "            # Run the optimizer, and specify the placeholders with the inputs and\n",
    "            # target outputs from the batch.\n",
    "            _, lossBatch = sess.run([optimizer, loss], feed_dict={inputNN: batchX, targetOutputNN: batchY})\n",
    "            # Keep track of the total loss over an entire epoch.\n",
    "            lossEpoch += lossBatch\n",
    "\n",
    "        if (epoch + 1) % 200 == 0:\n",
    "            # Output the loss over an epoch, every few epochs or so.\n",
    "            print(\"Epoch: %d - Average loss in last epoch = %1.1f\" % (epoch + 1, lossEpoch/nBatches))\n",
    "\n",
    "    print(\"Training process completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the accuracy of prediction via NN.\n",
    "\n",
    "Here, you report the mean absolute error of your predictions over the 'testX' dataset. Finally, plot the actual demand profile versus the predicted demand profile for a few days from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    predictedY, maeOfPrediction = sess.run([outputNN, maeY], feed_dict={inputNN: testX, targetOutputNN: testY})\n",
    "    print(\"Mean absolute error of forecast = \", maeOfPrediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question (5 points).** Comment whether your MAE is high or low.\n",
    "\n",
    "**Hint.** Compare the mean absolute error to the maximum demands.\n",
    "\n",
    "**Your answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us visualize the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Plot the predicted load and compare against the actual load from the test data.\n",
    "    assert(nTest >= 16)\n",
    "    days = random.sample(range(nTest), 16)\n",
    "\n",
    "    fig, axs = plt.subplots(4, 4, sharex=True, sharey=True, figsize=(10,10))\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    for dd, day in enumerate(days):\n",
    "        testYDay = testY[day]\n",
    "        predictedYDay = predictedY[day]\n",
    "\n",
    "        l1 = axs[dd].plot(range(1, 25), testYDay, label='Measured')\n",
    "        l2 = axs[dd].plot(range(1, 25), predictedYDay, label='Predicted')\n",
    "        \n",
    "        axs[dd].set_ylim(bottom=0, top=75000)\n",
    "        axs[dd].legend()\n",
    "    \n",
    "    fig.text(0.5, 0.07, 'Time of day (in hour)', ha='center')\n",
    "    fig.text(0.04, 0.5, 'Demand in Texas (in MW)', va='center', rotation='vertical')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question (20 points).** Explore how the number of epochs affects the accuracy and speed of training. Start with 10 epochs, and increase it to 100, 1000, 5000, 10000, and maybe more (do not exceed 20000 unless you have a powerful computer, you are only required to do up to 10000 for this lab). Make comments based on your observations. As an engineer, what is your favorite number of epochs, and why? \n",
    "\n",
    "**Your answer.**\n",
    "\n",
    "**Question (20 points).** Fix the number of echos to your favorite one, and then explore how the number of neurons affects the accuracy and speed of training. Start with 6 , and increase it to 12, 24, 48, 100, and more. Make comments based on your observations. As an engineer, what is your favorite number of neurons, and why? \n",
    "\n",
    "**Your answer.**\n",
    "\n",
    "**Question (30 points).** Fix the number of epochs and neurons to your favorite ones. Then, add another layer to the network. Discuss what your observe in terms of speed and accuracy. If the training becomes too slow, you may alter the number of epochs/neurons. \n",
    "\n",
    "**Your answer (comments here, code below). Your code should show the results for the 2 layers case. Go back to the codes above for the 1 layer case and run it again for the same number of epochs/neurons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The effect of wind energy (bonus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the raw data \n",
    "dfDemand = pd.read_csv(\"ERCOT_Hourly_Wind_Output_\" + str(year) + \".csv\")\n",
    "dfDemand[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in addition to the load data, we have some wind data! \n",
    "\n",
    "**Question (20 points).** Subtract the wind data from the load, and redo the above experiment and observe how does wind energy affect the forecasting process. How does the accuracy change? Why? Write down your MAE before and after considering wind energy. \n",
    "\n",
    "**Your answer (comments here, code below).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
